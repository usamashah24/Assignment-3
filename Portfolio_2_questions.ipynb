{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "Z88FfJc9lA_T",
      "metadata": {
        "id": "Z88FfJc9lA_T"
      },
      "source": [
        "## Analysis of an E-commerce Dataset Part 2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hoq0NwA9lA_V",
      "metadata": {
        "id": "hoq0NwA9lA_V"
      },
      "source": [
        "The goal of the second analysis task is to train linear regression models to predict users' ratings towards items. This involves a standard Data Science workflow: exploring data, building models, making predictions, and evaluating results. In this task, we will explore the impacts of feature selections and different sizes of training/testing data on the model performance. We will use another cleaned combined e-commerce sub-dataset that **is different from** the one in “Analysis of an E-commerce Dataset” task 1."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9fd3NU_lA_W",
      "metadata": {
        "id": "f9fd3NU_lA_W"
      },
      "source": [
        "### Import Cleaned E-commerce Dataset\n",
        "The csv file named 'cleaned_ecommerce_dataset.csv' is provided. You may need to use the Pandas method, i.e., `read_csv`, for reading it. After that, please print out its total length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "PJrb2gtAlA_W",
      "metadata": {
        "id": "PJrb2gtAlA_W"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total length: 2685\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('cleaned_ecommerce_dataset.csv')\n",
        "total_length = len(df)\n",
        "\n",
        "# Print the total length\n",
        "print(\"Total length:\", total_length)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aqbuU6rglA_X",
      "metadata": {
        "id": "aqbuU6rglA_X"
      },
      "source": [
        "### Explore the Dataset\n",
        "\n",
        "* Use the methods, i.e., `head()` and `info()`, to have a rough picture about the data, e.g., how many columns, and the data types of each column.\n",
        "* As our goal is to predict ratings given other columns, please get the correlations between helpfulness/gender/category/review and rating by using the `corr()` method.\n",
        "* To get the correlations between different features, you may need to first convert the categorical features (i.e., gender, category and review) into numerial values. For doing this, you may need to import `OrdinalEncoder` from `sklearn.preprocessing` (refer to the useful exmaples [here](https://pbpython.com/categorical-encoding.html))\n",
        "* Please provide ___necessary explanations/analysis___ on the correlations, and figure out which are the ___most___ and ___least___ corrleated features regarding rating. Try to ___discuss___ how the correlation will affect the final prediction results, if we use these features to train a regression model for rating prediction. In what follows, we will conduct experiments to verify your hypothesis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "W3PImHiElA_X",
      "metadata": {
        "id": "W3PImHiElA_X"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Length of Dataset: 2685\n",
            "   userId  timestamp                                           review  \\\n",
            "0    4081      71900                                Not always McCrap   \n",
            "1    4081      72000  I dropped the chalupa even before he told me to   \n",
            "2    4081      72000                     The Wonderful World of Wendy   \n",
            "3    4081     100399                             They actually did it   \n",
            "4    4081     100399                             Hey! Gimme some pie!   \n",
            "\n",
            "                                 item  rating  helpfulness gender  \\\n",
            "0                          McDonald's     4.0          3.0      M   \n",
            "1                           Taco Bell     1.0          4.0      M   \n",
            "2                             Wendy's     5.0          4.0      M   \n",
            "3  South Park: Bigger, Longer & Uncut     5.0          3.0      M   \n",
            "4                        American Pie     3.0          3.0      M   \n",
            "\n",
            "                category  item_id  item_price  user_city  \n",
            "0  Restaurants & Gourmet       41       30.74          4  \n",
            "1  Restaurants & Gourmet       74      108.30          4  \n",
            "2  Restaurants & Gourmet       84       69.00          4  \n",
            "3                 Movies       68      143.11          4  \n",
            "4                 Movies        6      117.89          4  \n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2685 entries, 0 to 2684\n",
            "Data columns (total 11 columns):\n",
            " #   Column       Non-Null Count  Dtype  \n",
            "---  ------       --------------  -----  \n",
            " 0   userId       2685 non-null   int64  \n",
            " 1   timestamp    2685 non-null   int64  \n",
            " 2   review       2685 non-null   object \n",
            " 3   item         2685 non-null   object \n",
            " 4   rating       2685 non-null   float64\n",
            " 5   helpfulness  2685 non-null   float64\n",
            " 6   gender       2685 non-null   object \n",
            " 7   category     2685 non-null   object \n",
            " 8   item_id      2685 non-null   int64  \n",
            " 9   item_price   2685 non-null   float64\n",
            " 10  user_city    2685 non-null   int64  \n",
            "dtypes: float64(3), int64(4), object(4)\n",
            "memory usage: 230.9+ KB\n",
            "None\n",
            "             helpfulness    gender  category    review    rating\n",
            "helpfulness     1.000000  0.075947 -0.013408 -0.028259 -0.007523\n",
            "gender          0.075947  1.000000  0.022549 -0.037884 -0.034337\n",
            "category       -0.013408  0.022549  1.000000  0.001970 -0.163158\n",
            "review         -0.028259 -0.037884  0.001970  1.000000 -0.036118\n",
            "rating         -0.007523 -0.034337 -0.163158 -0.036118  1.000000\n",
            "Most correlated features with 'rating':\n",
            "rating         1.000000\n",
            "helpfulness   -0.007523\n",
            "gender        -0.034337\n",
            "review        -0.036118\n",
            "category      -0.163158\n",
            "Name: rating, dtype: float64\n",
            "Least correlated features with 'rating':\n",
            "category      -0.163158\n",
            "review        -0.036118\n",
            "gender        -0.034337\n",
            "helpfulness   -0.007523\n",
            "rating         1.000000\n",
            "Name: rating, dtype: float64\n",
            "Understanding Correlations:\n",
            "Correlation values range from -1 to 1.\n",
            "When there's a positive correlation, if one thing increases, the other usually increases as well.\n",
            "With negative correlation, if one thing goes up, the other often goes down.\n",
            "When it comes to predicting 'rating':\n",
            "Features that have a strong positive correlation might have a big impact on predicting higher ratings.\n",
            "On the flip side, features with a strong negative correlation might really matter when it comes to predicting lower ratings.\n",
            "But remember, just because things are correlated doesn't mean one causes the other. We need to do more experiments to confirm our ideas.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "\n",
        "df = pd.read_csv('cleaned_ecommerce_dataset.csv')\n",
        "\n",
        "# Print the total length \n",
        "print(f'Total Length of Dataset: {len(df)}')\n",
        "\n",
        "# Explore the dataset\n",
        "print(df.head())\n",
        "print(df.info())\n",
        "\n",
        "# Convert categorical features (gender, category, review) into numerical values\n",
        "encoder = OrdinalEncoder()\n",
        "df[['gender', 'category', 'review']] = encoder.fit_transform(df[['gender', 'category', 'review']])\n",
        "\n",
        "# Get correlations between helpfulness, gender, category, review, and rating\n",
        "correlations = df[['helpfulness', 'gender', 'category', 'review', 'rating']].corr()\n",
        "\n",
        "# Print the correlation matrix\n",
        "print(correlations)\n",
        "\n",
        "# Analyze correlations\n",
        "print(\"Most correlated features with 'rating':\")\n",
        "print(correlations['rating'].sort_values(ascending=False))\n",
        "\n",
        "print(\"Least correlated features with 'rating':\")\n",
        "print(correlations['rating'].sort_values(ascending=True))\n",
        "\n",
        "# Let's talk about how correlations affect our predictions.\n",
        "print(\"Understanding Correlations:\")\n",
        "\n",
        "print(\"Correlation values range from -1 to 1.\")\n",
        "\n",
        "print(\"When there's a positive correlation, if one thing increases, the other usually increases as well.\")\n",
        "\n",
        "\n",
        "print(\"With negative correlation, if one thing goes up, the other often goes down.\")\n",
        "\n",
        "\n",
        "print(\"When it comes to predicting 'rating':\")\n",
        "\n",
        "\n",
        "print(\"Features that have a strong positive correlation might have a big impact on predicting higher ratings.\")\n",
        "\n",
        "\n",
        "print(\"On the flip side, features with a strong negative correlation might really matter when it comes to predicting lower ratings.\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4myP5igslA_Y",
      "metadata": {
        "id": "4myP5igslA_Y"
      },
      "source": [
        "### Split Training and Testing Data\n",
        "* Machine learning models are trained to help make predictions for the future. Normally, we need to randomly split the dataset into training and testing sets, where we use the training set to train the model, and then leverage the well-trained model to make predictions on the testing set.\n",
        "* To further investigate whether the size of the training/testing data affects the model performance, please random split the data into training and testing sets with different sizes:\n",
        "    * Case 1: training data containing 10% of the entire data;\n",
        "    * Case 2: training data containing 90% of the entire data.\n",
        "* Print the shape of training and testing sets in the two cases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "JIDMig9blA_Y",
      "metadata": {
        "id": "JIDMig9blA_Y"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Case 1 - 10% Training Data:\n",
            "Training Set Shape: (268, 11)\n",
            "Testing Set Shape: (2417, 11)\n",
            "\n",
            "Case 2 - 90% Training Data:\n",
            "Training Set Shape: (2416, 11)\n",
            "Testing Set Shape: (269, 11)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df = pd.read_csv('cleaned_ecommerce_dataset.csv')\n",
        "\n",
        "# Case 1: Training data containing 10%\n",
        "train_case1, test_case1 = train_test_split(df, test_size=0.9, random_state=142)\n",
        "\n",
        "# Case 2: Training data containing 90%\n",
        "train_case2, test_case2 = train_test_split(df, test_size=0.1, random_state=142)\n",
        "\n",
        "# Print the shapes of training and testing sets for both cases\n",
        "print(\"Case 1 - 10% Training Data:\")\n",
        "print(\"Training Set Shape:\", train_case1.shape)\n",
        "print(\"Testing Set Shape:\", test_case1.shape)\n",
        "\n",
        "print(\"\\nCase 2 - 90% Training Data:\")\n",
        "print(\"Training Set Shape:\", train_case2.shape)\n",
        "print(\"Testing Set Shape:\", test_case2.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DjSsgT0BlA_Y",
      "metadata": {
        "id": "DjSsgT0BlA_Y"
      },
      "source": [
        "### Train Linear Regression Models with Feature Selection under Cases 1 & 2\n",
        "* When training a machine learning model for prediction, we may need to select the most important/correlated input features for more accurate results.\n",
        "* To investigate whether feature selection affects the model performance, please select two most correlated features and two least correlated features regarding rating, respectively.\n",
        "* Train four linear regression models by following the conditions:\n",
        "    - (model-a) using the training/testing data in case 1 with two most correlated input features\n",
        "    - (model-b) using the training/testing data in case 1 with two least correlated input features\n",
        "    - (model-c) using the training/testing data in case 2 with two most correlated input features\n",
        "    - (model-d) using the training/testing data in case 2 with two least correlated input features\n",
        "* By doing this, we can verify the impacts of the size of traing/testing data on the model performance via comparing model-a and model-c (or model-b and model-d); meanwhile the impacts of feature selection can be validated via comparing model-a and model-b (or model-c and model-d).    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "DASzPUATlA_Z",
      "metadata": {
        "id": "DASzPUATlA_Z"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model-a (Most Correlated, Case 1) MSE: 1.8330766677280501\n",
            "Model-b (Least Correlated, Case 1) MSE: 1.8415001349613145\n",
            "Model-c (Most Correlated, Case 2) MSE: 1.7991798840230246\n",
            "Model-d (Least Correlated, Case 2) MSE: 1.8019679273170766\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('cleaned_ecommerce_dataset.csv')\n",
        "\n",
        "# Define the columns for feature selection\n",
        "most_correlated_features = ['user_city', 'item_id']  \n",
        "least_correlated_features = ['timestamp', 'helpfulness'] \n",
        "\n",
        "\n",
        "# Case 1: Training data containing 10%\n",
        "train_case1, test_case1 = train_test_split(df, test_size=0.9, random_state=142)\n",
        "\n",
        "# Case 2: Training data containing 90%\n",
        "train_case2, test_case2 = train_test_split(df, test_size=0.1, random_state=142)\n",
        "\n",
        "# Define linear regression models\n",
        "model_a_case1 = LinearRegression()\n",
        "model_b_case1 = LinearRegression()\n",
        "model_c_case2 = LinearRegression()\n",
        "model_d_case2 = LinearRegression()\n",
        "\n",
        "# Train model-a with the two most correlated features using case 1 data\n",
        "X_model_a_case1 = train_case1[most_correlated_features]\n",
        "y_model_a_case1 = train_case1['rating']\n",
        "model_a_case1.fit(X_model_a_case1, y_model_a_case1)\n",
        "\n",
        "# Train model-b with the two least correlated features using case 1 data\n",
        "X_model_b_case1 = train_case1[least_correlated_features]\n",
        "y_model_b_case1 = train_case1['rating']\n",
        "model_b_case1.fit(X_model_b_case1, y_model_b_case1)\n",
        "\n",
        "# Train model-c with the two most correlated features using case 2 data\n",
        "X_model_c_case2 = train_case2[most_correlated_features]\n",
        "y_model_c_case2 = train_case2['rating']\n",
        "model_c_case2.fit(X_model_c_case2, y_model_c_case2)\n",
        "\n",
        "# Train model-d with the two least correlated features using case 2 data\n",
        "X_model_d_case2 = train_case2[least_correlated_features]\n",
        "y_model_d_case2 = train_case2['rating']\n",
        "model_d_case2.fit(X_model_d_case2, y_model_d_case2)\n",
        "\n",
        "# Make predictions and evaluate models\n",
        "y_pred_model_a = model_a_case1.predict(test_case1[most_correlated_features])\n",
        "mse_model_a = mean_squared_error(test_case1['rating'], y_pred_model_a)\n",
        "\n",
        "y_pred_model_b = model_b_case1.predict(test_case1[least_correlated_features])\n",
        "mse_model_b = mean_squared_error(test_case1['rating'], y_pred_model_b)\n",
        "\n",
        "y_pred_model_c = model_c_case2.predict(test_case2[most_correlated_features])\n",
        "mse_model_c = mean_squared_error(test_case2['rating'], y_pred_model_c)\n",
        "\n",
        "y_pred_model_d = model_d_case2.predict(test_case2[least_correlated_features])\n",
        "mse_model_d = mean_squared_error(test_case2['rating'], y_pred_model_d)\n",
        "\n",
        "# Print MSE values for evaluation\n",
        "print(\"Model-a (Most Correlated, Case 1) MSE:\", mse_model_a)\n",
        "print(\"Model-b (Least Correlated, Case 1) MSE:\", mse_model_b)\n",
        "print(\"Model-c (Most Correlated, Case 2) MSE:\", mse_model_c)\n",
        "print(\"Model-d (Least Correlated, Case 2) MSE:\", mse_model_d)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KATSn7hYlA_Z",
      "metadata": {
        "id": "KATSn7hYlA_Z"
      },
      "source": [
        "### Evaluate Models\n",
        "* Evaluate the performance of the four models with two metrics, including MSE and Root MSE\n",
        "* Print the results of the four models regarding the two metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fU8GPS9lA_Z",
      "metadata": {
        "id": "4fU8GPS9lA_Z"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "Y9jx-eY6lA_a",
      "metadata": {
        "id": "Y9jx-eY6lA_a"
      },
      "source": [
        "### Visualize, Compare and Analyze the Results\n",
        "* Visulize the results, and perform ___insightful analysis___ on the obtained results. For better visualization, you may need to carefully set the scale for the y-axis.\n",
        "* Normally, the model trained with most correlated features and more training data will get better results. Do you obtain the similar observations? If not, please ___explain the possible reasons___."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3TNAIGDilA_a",
      "metadata": {
        "id": "3TNAIGDilA_a"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.11.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "03100797071d81d2fc1614d8d4a7b85db30280222e7da549502335b5750ba344"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
